{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwYm2WMhkjSjvOx6xCJ7W3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acsontfd/NewsScrapers/blob/main/assessment_CGT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install google-colab-selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yUuyRmM5DO9j",
        "outputId": "4e9858ba-8c94-43be-9b7b-71b0244922bc"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-colab-selenium in /usr/local/lib/python3.11/dist-packages (1.0.14)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (from google-colab-selenium) (4.29.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (2.3.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (25.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium->google-colab-selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->google-colab-selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install google-colab-selenium[undetected]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NjyWlFDTEW6c",
        "outputId": "e6c18329-112e-4b3e-dfa7-330e1686279a"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-colab-selenium[undetected] in /usr/local/lib/python3.11/dist-packages (1.0.14)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (from google-colab-selenium[undetected]) (4.29.0)\n",
            "Requirement already satisfied: undetected-chromedriver in /usr/local/lib/python3.11/dist-packages (from google-colab-selenium[undetected]) (3.5.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium[undetected]) (2.3.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium[undetected]) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium[undetected]) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium[undetected]) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium[undetected]) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium[undetected]) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from undetected-chromedriver->google-colab-selenium[undetected]) (2.32.3)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from undetected-chromedriver->google-colab-selenium[undetected]) (14.2)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium[undetected]) (25.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium[undetected]) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium[undetected]) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium[undetected]) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium[undetected]) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium->google-colab-selenium[undetected]) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium[undetected]) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->undetected-chromedriver->google-colab-selenium[undetected]) (3.4.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->google-colab-selenium[undetected]) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install selenium webdriver-manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIPlHKyRfD3n",
        "outputId": "b8bafedf-68f4-4ee5-aef1-350b1402cf72"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.29.0)\n",
            "Requirement already satisfied: webdriver-manager in /usr/local/lib/python3.11/dist-packages (4.0.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.29.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (1.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import google_colab_selenium as gs\n",
        "from datetime import datetime, timedelta\n",
        "import re\n",
        "\n",
        "driver = gs.Chrome()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "JDwdJlJXDc1y",
        "outputId": "ae7624a6-35ff-4186-85b0-4aee3481bdc7"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"c25e99c5-74bc-4fb1-805c-51f3a4ace9a1-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"c25e99c5-74bc-4fb1-805c-51f3a4ace9a1-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "            const element = document.getElementById(\"c25e99c5-74bc-4fb1-805c-51f3a4ace9a1-circle\");\n",
              "            element.style.border = \"3px solid limegreen\";\n",
              "            element.style.animation = \"none\";\n",
              "\n",
              "            const text = document.getElementById(\"c25e99c5-74bc-4fb1-805c-51f3a4ace9a1-text\");\n",
              "            text.innerText = \"Initialized Chromedriver\";\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_published_date(relative_time):\n",
        "    \"\"\"Convert relative time (e.g., '2 hours ago') to actual date.\"\"\"\n",
        "    now = datetime.now()\n",
        "\n",
        "    parts = relative_time.split()\n",
        "    if len(parts) < 2:\n",
        "        return now.strftime(\"%Y-%m-%d %H:%M\")  # Return current time if format is unknown\n",
        "\n",
        "    num, unit = int(parts[0]), parts[1]\n",
        "\n",
        "    # Convert different time formats\n",
        "    if \"minute\" in unit:\n",
        "        return (now - timedelta(minutes=num)).strftime(\"%Y-%m-%d %H:%M\")\n",
        "    elif \"hour\" in unit:\n",
        "        return (now - timedelta(hours=num)).strftime(\"%Y-%m-%d %H:%M\")\n",
        "    elif \"day\" in unit:\n",
        "        return (now - timedelta(days=num)).strftime(\"%Y-%m-%d\")\n",
        "    elif \"week\" in unit:\n",
        "        return (now - timedelta(weeks=num)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    return now.strftime(\"%Y-%m-%d\")  # Default case"
      ],
      "metadata": {
        "id": "edcrtrh57a_i"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_date(date_text):\n",
        "    \"\"\"Check if the extracted text follows a valid date format.\"\"\"\n",
        "    date_pattern = r\"^\\d{1,2} [A-Za-z]+( \\d{4})?$\"  # Matches \"11 Feb\", \"23 February\", or \"17 December 2024\"\n",
        "    return bool(re.match(date_pattern, date_text))"
      ],
      "metadata": {
        "id": "PYB6XQNTIZtb"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bbc_articles():\n",
        "    URL = 'https://www.bbc.com/news'\n",
        "    response = requests.get(URL)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(\"The response code is:\", response.status_code)\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        articles_data = []\n",
        "        headlines = soup.find_all(\"h2\", {\"data-testid\": \"card-headline\"})\n",
        "        descriptions = soup.find_all(\"p\", {\"data-testid\": \"card-description\"})\n",
        "        published_dates = soup.find_all(\"span\", {\"data-testid\": \"card-metadata-lastupdated\"})\n",
        "\n",
        "        for i in range(len(headlines)):\n",
        "            title = headlines[i].text.strip()\n",
        "            description = descriptions[i].text.strip() if i < len(descriptions) else \"No description\"\n",
        "            relative_time = published_dates[i].text.strip() if i < len(published_dates) else \"No date\"\n",
        "\n",
        "            # Ensure relative_time is defined before using it\n",
        "            if \"ago\" in relative_time:\n",
        "                published_date = get_published_date(relative_time)\n",
        "            else:\n",
        "                published_date = relative_time  # Keep the original date if not relative\n",
        "\n",
        "            articles_data.append({\n",
        "                \"published_date\": published_date,\n",
        "                \"headline\": title,\n",
        "                \"publisher\": \"BBC\",\n",
        "                \"article_content\": description,  # Short description as a placeholder for full content\n",
        "                \"category\": \"General\"  # Needs to be mapped from URL or sections\n",
        "            })\n",
        "\n",
        "        print(f\"BBC articles scraped: {len(articles_data)}\")\n",
        "        return articles_data\n",
        "    else:\n",
        "        print(f\"Failed to retrieve BBC news. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# Run function\n",
        "bbc_news = bbc_articles()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxjEHxdlYQfz",
        "outputId": "7d9055c3-4a62-414b-f9db-ce9db8400d25"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: 200\n",
            "BBC articles scraped: 61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bbc_sports_articles():\n",
        "    URL = 'https://www.bbc.com/sport'\n",
        "    response = requests.get(URL)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(\"The response code is:\", response.status_code)\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        articles_data = []\n",
        "        headlines = soup.find_all(\"p\", class_=\"ssrcss-1b1mki6-PromoHeadline\")  # Extract headlines\n",
        "        descriptions = soup.find_all(\"p\", {\"data-testid\": \"card-description\"})\n",
        "        published_dates = soup.find_all(\"span\", class_=\"ssrcss-1f39n02-VisuallyHidden\")  # Extract actual date\n",
        "\n",
        "        for i in range(len(headlines)):\n",
        "            title = headlines[i].text.strip()\n",
        "            description = descriptions[i].text.strip() if i < len(descriptions) else \"No description\"\n",
        "            extracted_date = published_dates[i].text.strip() if i < len(published_dates) else \"No date\"\n",
        "\n",
        "            # Process extracted date\n",
        "            if \"ago\" in extracted_date:\n",
        "                published_date = get_published_date(extracted_date)  # Convert relative time\n",
        "            elif is_valid_date(extracted_date):\n",
        "                try:\n",
        "                    # Convert absolute date to YYYY-MM-DD if year is present\n",
        "                    if len(extracted_date.split()) == 3:\n",
        "                        published_date = datetime.strptime(extracted_date, \"%d %B %Y\").strftime(\"%Y-%m-%d\")\n",
        "                    else:\n",
        "                        # Keep as-is for formats like \"23 February\"\n",
        "                        published_date = extracted_date\n",
        "                except ValueError:\n",
        "                    published_date = extracted_date  # If conversion fails, store as-is\n",
        "            else:\n",
        "                print(f\"Skipping invalid date: {extracted_date}\")\n",
        "                continue  # Skip if it's not a valid date\n",
        "\n",
        "            articles_data.append({\n",
        "                \"published_date\": published_date,\n",
        "                \"headline\": title,\n",
        "                \"publisher\": \"BBC\",\n",
        "                \"article_content\": description,\n",
        "                \"category\": \"Sports\"\n",
        "            })\n",
        "\n",
        "        print(f\"BBC Sports articles scraped: {len(articles_data)}\")\n",
        "        return articles_data\n",
        "    else:\n",
        "        print(f\"Failed to retrieve BBC Sports news. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# Run function\n",
        "bbc_sports_news = bbc_sports_articles()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1PPCaDd9s6B",
        "outputId": "5297499a-cb02-4868-c8d9-1782e35c577f"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: 200\n",
            "Skipping invalid date: BBC Homepage\n",
            "Skipping invalid date: More menu\n",
            "Skipping invalid date: More menu\n",
            "Skipping invalid date: Close menu\n",
            "Skipping invalid date: BBC Sport\n",
            "Skipping invalid date: .\n",
            "Skipping invalid date: .\n",
            "Skipping invalid date: .\n",
            "Skipping invalid date: Watch: YouTuber MrBeast crashes Formula E car. Video, 00:00:30\n",
            "Skipping invalid date: Uncle Harry is 'the best supporter ever' - Lampard. Video, 00:02:01\n",
            "Skipping invalid date: Mourinho falls asleep during reporter's long question. Video, 00:00:34\n",
            "Skipping invalid date: Chelsea must finish in top four - Colwill. Video, 00:04:03\n",
            "Skipping invalid date: 'Arriving in red will be cool' - Hamilton's Melbourne excitement. Video, 00:01:12\n",
            "Skipping invalid date: .\n",
            "Skipping invalid date: Russell won't 'bow down' to Verstappen. Video, 00:02:35\n",
            "Skipping invalid date: Ferguson 'not carried away after brilliant result' Video, 00:00:58\n",
            "Skipping invalid date: 'Liverpool must improve in PSG second leg' Video, 00:02:17\n",
            "Skipping invalid date: 'How on earth do you leave him out?' - the Kane debate. Video, 00:01:10\n",
            "Skipping invalid date: Jonas' trainer unhappy with 'uncalibrated' scales. Video, 00:01:17\n",
            "Skipping invalid date: Football Daily. In Focus with Frank Lampard and Dion Dublin. Audio, 22 minutes\n",
            "Skipping invalid date: Rugby Union Weekly. Six Nations: Ireland in Dublin, the ultimate challenge? Audio, 34 minutes\n",
            "BBC Sports articles scraped: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bbc_business_articles():\n",
        "    URL = 'https://www.bbc.com/news/business'\n",
        "    response = requests.get(URL)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(\"The response code is:\", response.status_code)\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        articles_data = []\n",
        "        headlines = soup.find_all(\"h2\", {\"data-testid\": \"card-headline\"})\n",
        "        descriptions = soup.find_all(\"p\", {\"data-testid\": \"card-description\"})\n",
        "        published_dates = soup.find_all(\"span\", {\"data-testid\": \"card-metadata-lastupdated\"})\n",
        "\n",
        "        for i in range(len(headlines)):\n",
        "            title = headlines[i].text.strip()\n",
        "            description = descriptions[i].text.strip() if i < len(descriptions) else \"No description\"\n",
        "            relative_time = published_dates[i].text.strip() if i < len(published_dates) else \"No date\"\n",
        "\n",
        "            # Ensure relative_time is defined before using it\n",
        "            if \"ago\" in relative_time:\n",
        "                published_date = get_published_date(relative_time)\n",
        "            else:\n",
        "                published_date = relative_time  # Keep the original date if not relative\n",
        "\n",
        "            articles_data.append({\n",
        "                \"published_date\": published_date,\n",
        "                \"headline\": title,\n",
        "                \"publisher\": \"BBC\",\n",
        "                \"article_content\": description,\n",
        "                \"category\": \"Business\"\n",
        "            })\n",
        "\n",
        "        print(f\"BBC Business articles scraped: {len(articles_data)}\")\n",
        "        return articles_data\n",
        "    else:\n",
        "        print(f\"Failed to retrieve BBC Business news. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# Run function\n",
        "bbc_business_news = bbc_business_articles()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9zLLW9XB3S2",
        "outputId": "93293f4f-579b-4d24-fb21-7ac27b6189de"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The response code is: 200\n",
            "BBC Business articles scraped: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_thestar_nation():\n",
        "    URL = \"https://www.thestar.com.my/news/nation\"\n",
        "\n",
        "    # Set up Selenium WebDriver\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--headless\")  # Run in headless mode (optional)\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    # Use webdriver_manager to install ChromeDriver automatically\n",
        "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "    driver.get(URL)\n",
        "    wait = WebDriverWait(driver, 10)\n",
        "\n",
        "    max_clicks = 10\n",
        "    click_count = 0\n",
        "\n",
        "    while click_count < max_clicks:\n",
        "        try:\n",
        "            # Scroll down to make the \"Load More\" button visible\n",
        "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "            time.sleep(1.5)\n",
        "\n",
        "            # Find and click the \"Load More\" button\n",
        "            load_more_button = wait.until(EC.element_to_be_clickable((By.ID, \"loadMorestories\")))\n",
        "            driver.execute_script(\"arguments[0].click();\", load_more_button)\n",
        "\n",
        "            click_count += 1\n",
        "            print(f\"Clicked Load More {click_count} time(s)...\")\n",
        "\n",
        "            # Wait for new articles to load\n",
        "            time.sleep(2)\n",
        "\n",
        "        except Exception:\n",
        "            print(\"No more articles to load or Load More button not found.\")\n",
        "            break  # Exit loop when no more articles\n",
        "\n",
        "    # Parse page source with BeautifulSoup\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    articles_data = []\n",
        "    news_items = soup.find_all(\"div\", class_=\"row list-listing\")  # Each news item\n",
        "\n",
        "    for item in news_items:\n",
        "        # Extract headline\n",
        "        headline_tag = item.find(\"h2\", class_=\"f18\")\n",
        "        headline = headline_tag.a.text.strip() if headline_tag and headline_tag.a else \"No headline\"\n",
        "\n",
        "        # Extract description\n",
        "        description_tag = item.find(\"p\", style=\"overflow-wrap: break-word;\")\n",
        "        description = description_tag.text.strip() if description_tag else \"No description\"\n",
        "\n",
        "        # Extract timestamp\n",
        "        timestamp_tag = item.find(\"label\", class_=\"timestamp\")\n",
        "        published_date = timestamp_tag.text.strip() if timestamp_tag else \"No date\"\n",
        "\n",
        "        # Convert relative timestamps like \"7m ago\"\n",
        "        if \"ago\" in published_date:\n",
        "            published_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        articles_data.append({\n",
        "            \"published_date\": published_date,\n",
        "            \"headline\": headline,\n",
        "            \"publisher\": \"The Star\",\n",
        "            \"article_content\": description,\n",
        "            \"category\": \"Nation\",\n",
        "        })\n",
        "\n",
        "    print(f\"Total articles scraped: {len(articles_data)}\")\n",
        "\n",
        "    # Close the browser\n",
        "    driver.quit()\n",
        "\n",
        "    return articles_data\n",
        "\n",
        "# Run scraper\n",
        "thestar_nation_news = scrape_thestar_nation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD1qrfUotW-r",
        "outputId": "96143f72-60e8-493d-a988-6e008b29c5f8"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clicked Load More 1 time(s)...\n",
            "Clicked Load More 2 time(s)...\n",
            "Clicked Load More 3 time(s)...\n",
            "Clicked Load More 4 time(s)...\n",
            "Clicked Load More 5 time(s)...\n",
            "Clicked Load More 6 time(s)...\n",
            "Clicked Load More 7 time(s)...\n",
            "Clicked Load More 8 time(s)...\n",
            "Clicked Load More 9 time(s)...\n",
            "Clicked Load More 10 time(s)...\n",
            "Total articles scraped: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    driver.quit()\n"
      ],
      "metadata": {
        "id": "1Xfv-fBFQg3w"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_thestar_business():\n",
        "    URL = \"https://www.thestar.com.my/news/latest?tag=Business\"\n",
        "\n",
        "    # Set up Selenium WebDriver\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--headless\")  # Run in headless mode (optional)\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "    driver.get(URL)\n",
        "    wait = WebDriverWait(driver, 10)\n",
        "\n",
        "    max_pages = 3  # Scrape page 1, then click page 2 and 3\n",
        "    current_page = 1\n",
        "    articles_data = []\n",
        "\n",
        "    while current_page <= max_pages:\n",
        "        print(f\"Scraping page {current_page}...\")\n",
        "\n",
        "        # Parse page source with BeautifulSoup\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "        news_items = soup.find_all(\"li\", class_=\"row\")  # Each news item\n",
        "\n",
        "        for item in news_items:\n",
        "            # Extract headline\n",
        "            headline_tag = item.find(\"h2\", class_=\"f18\")\n",
        "            headline = headline_tag.a.text.strip() if headline_tag and headline_tag.a else \"No headline\"\n",
        "\n",
        "            # Extract description\n",
        "            description_tag = item.find(\"p\", style=\"overflow-wrap: break-word;\")\n",
        "            description = description_tag.text.strip() if description_tag else \"No description\"\n",
        "\n",
        "            # Extract timestamp\n",
        "            timestamp_tag = item.find(\"time\", class_=\"timestamp\")\n",
        "            published_date = timestamp_tag.text.strip() if timestamp_tag else \"No date\"\n",
        "\n",
        "            # Convert relative timestamps like \"7m ago\"\n",
        "            if \"ago\" in published_date:\n",
        "                published_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "            articles_data.append({\n",
        "                \"published_date\": published_date,\n",
        "                \"headline\": headline,\n",
        "                \"publisher\": \"The Star\",\n",
        "                \"article_content\": description,\n",
        "                \"category\": \"Business\",\n",
        "            })\n",
        "\n",
        "        # Click on the next page number\n",
        "        try:\n",
        "            next_page_number = str(current_page + 1)  # Convert to string\n",
        "            next_page_button = wait.until(EC.element_to_be_clickable((By.LINK_TEXT, next_page_number)))\n",
        "            next_page_button.click()\n",
        "            time.sleep(3)  # Allow page to load\n",
        "            current_page += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Could not find page {current_page + 1}: {e}\")\n",
        "            break  # Stop if there's no more pages to click\n",
        "\n",
        "    print(f\"Total articles scraped: {len(articles_data)}\")\n",
        "\n",
        "    # Close the browser\n",
        "    driver.quit()\n",
        "\n",
        "    return articles_data\n",
        "\n",
        "# Run scraper\n",
        "thestar_business_news = scrape_thestar_business()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUy0zfOXCauG",
        "outputId": "1b296b96-85df-45fe-d96f-2c0685d183cb"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Could not find page 3: HTTPConnectionPool(host='localhost', port=35889): Read timed out. (read timeout=120)\n",
            "Total articles scraped: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_nst_nation():\n",
        "    URL = \"https://www.nst.com.my/news/nation\"\n",
        "\n",
        "    # Set up Selenium WebDriver\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument(\"--headless\")  # Run in headless mode (optional)\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "    driver.get(URL)\n",
        "    wait = WebDriverWait(driver, 10)\n",
        "\n",
        "    # Allow time for JS to load more articles\n",
        "    time.sleep(10)\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "    # Locate all news articles (corrected selector)\n",
        "    news_items = soup.find_all(\"div\", class_=\"col\")  # Updated selector\n",
        "\n",
        "    articles_data = []\n",
        "    for item in news_items:\n",
        "        # Extract headline\n",
        "        headline_tag = item.find(\"h2\", class_=\"node-title\")\n",
        "        headline = headline_tag.text.strip() if headline_tag else \"No headline\"\n",
        "\n",
        "        # Extract description/snippet\n",
        "        description_tag = item.find(\"div\", class_=\"field-content\")\n",
        "        description = description_tag.text.strip() if description_tag else \"No description\"\n",
        "\n",
        "        # Extract timestamp\n",
        "        timestamp_tag = item.find(\"time\", class_=\"created\")\n",
        "        published_date = timestamp_tag.text.strip() if timestamp_tag else \"No date\"\n",
        "\n",
        "        # Convert relative timestamps like \"7m ago\"\n",
        "        if \"ago\" in published_date:\n",
        "            published_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        articles_data.append({\n",
        "            \"published_date\": published_date,\n",
        "            \"headline\": headline,\n",
        "            \"publisher\": \"NST\",\n",
        "            \"article_content\": description,\n",
        "            \"category\": \"Nation\",\n",
        "        })\n",
        "\n",
        "    print(f\"Total articles scraped: {len(articles_data)}\")\n",
        "\n",
        "    driver.quit()\n",
        "    return articles_data\n",
        "\n",
        "# Run scraper\n",
        "nst_nation_news = scrape_nst_nation()"
      ],
      "metadata": {
        "id": "XKOaHRN7Wxtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to DataFrame\n",
        "df_bbc = pd.DataFrame(bbc_news\n",
        "                      + bbc_sports_news\n",
        "                      + bbc_business_news\n",
        "                      + thestar_nation_news\n",
        "                      + thestar_business_news\n",
        "                      + nst_nation_news)\n",
        "\n",
        "# Save to CSV (optional)\n",
        "df_bbc.to_csv(\"news.csv\", index=False)\n",
        "\n",
        "# Display DataFrame\n",
        "print(df_bbc.shape)\n",
        "df_bbc.head()"
      ],
      "metadata": {
        "id": "5IKeXkWOrDlL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}